from abc import abstractmethod
import random
import tensorflow as tf
import cytoolz as cz
import itertools as it
from copy import copy
from tfinterface.decorators import return_self, with_graph_as_default
from tfinterface.metrics import r2_score, sigmoid_score, softmax_score

from tfinterface.base import Trainer


class SupervisedTrainer(Trainer):
    """
# Inteface
* `model : SupervisedModel` -
* `loss : Tensor` -
* `update : Tensor` -
    """
    def __init__(self, name, loss="MSE", optimizer=tf.train.AdamOptimizer, learning_rate=0.001, **kwargs):
        super(SupervisedTrainer, self).__init__(name, **kwargs)

        self._loss_arg = loss
        self._optimizer = optimizer
        self._learning_rate_arg = learning_rate

    def _build(self, model):
        # copy model to remain semi-immutable
        self.model = copy(model)

        inputs = self.model.inputs

        if not hasattr(self.model, "labels"):
            self.model.labels = inputs.labels


        if hasattr(self.model, "learning_rate"):
            learning_rate = self.model.learning_rate
        elif hasattr(inputs, "learning_rate"):
            learning_rate = inputs.learning_rate
        else:
            learning_rate = self._learning_rate_arg


        if not hasattr(self.model, "loss"):
            loss = self._loss_arg

            if loss == "MSE":
                self.model.loss = tf.nn.l2_loss(model.predictions, self.model.labels) |> tf.reduce_mean
            elif loss == "sigmoid":
                self.model.loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=model.logits, labels=self.model.labels) |> tf.reduce_mean
            elif loss == "softmax":
                self.model.loss = tf.nn.softmax_cross_entropy_with_logits(logits=model.logits, labels=self.model.labels) |> tf.reduce_mean


        if not hasattr(self.model, "score"):
            loss = self._loss_arg

            if loss == "MSE":
                self.model.score = r2_score(self.model.labels, self.model.predictions)
            elif loss == "sigmoid":
                self.model.score = sigmoid_score(self.model.labels, self.model.predictions)
            elif loss == "softmax":
                self.model.score = softmax_score(self.model.labels, self.model.predictions)

        if not hasattr(self.model, "update"):
            self.model.update = self._optimizer(learning_rate).minimize(self.model.loss)


    def predict(self, *args, **kwargs):
        return self.model.predict(*args, **kwargs)

    def score(self, **kwargs):
        fit_feed = self.model.inputs.fit_feed(**kwargs)
        score = self.sess.run(self.model.score, feed_dict=fit_feed)

        return score

    @return_self
    @with_graph_as_default
    def fit(self, epochs=2000, data_generator=None, log_summaries=False, writer_kwargs={}):
        if log_summaries and not hasattr(self, "writer"):
            self.writer = tf.summary.FileWriter(self.logs_path, graph=self.graph, **writer_kwargs)

        if not hasattr(self, "summaries"):
            self.summaries = tf.no_op()

        if data_generator is None:
            #generator of empty dicts
            data_generator = it.repeat({})

        data_generator = data_generator |> cz.take$(epochs)

        for i, batch_feed_data in enumerate(data_generator):

            fit_feed = self.model.inputs.fit_feed(**batch_feed_data)
            _, summaries = self.sess.run([self.model.update, self.summaries], feed_dict=fit_feed)

            if log_summaries and summaries is not None:
                self.writer.add_summary(summaries, global_step=global_step)


            ## REMOVE NEXT, ONLY FOR TESTING
            loss, score = self.sess.run([self.model.loss, self.model.score], feed_dict=fit_feed)
            print("loss {}, score {}, at {}".format(loss, score, i))
