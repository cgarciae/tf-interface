from abc import abstractmethod
import random
import tensorflow as tf
import cytoolz as cz
import itertools as it
from copy import copy
from tfinterface.decorators import return_self, with_graph_as_default

from tfinterface.base import Trainer


class SupervisedTrainer(Trainer):
    """
# Inteface
* `model : SupervisedModel` -
* `loss : Tensor` -
* `update : Tensor` -
    """
    def __init__(self, name, loss="mse", optimizer=tf.train.AdamOptimizer, learning_rate=0.001, **kwargs):
        super(SupervisedTrainer, self).__init__(name, **kwargs)

        self._loss_arg = loss
        self._optimizer = optimizer
        self._learning_rate_arg = learning_rate

    def _build(self, model):
        # shallow copy model to remain semi-immutable
        self.model = copy(model)



    @return_self
    @with_graph_as_default
    def fit(self, epochs=2000, data_generator=None, log_summaries=False, writer_kwargs={}):
        if log_summaries and not hasattr(self, "writer"):
            self.writer = tf.summary.FileWriter(self.logs_path, graph=self.graph, **writer_kwargs)

        if not hasattr(self, "summaries"):
            self.summaries = tf.no_op()

        if data_generator is None:
            #generator of empty dicts
            data_generator = it.repeat({})

        data_generator = data_generator |> cz.take$(epochs)

        for i, batch_feed_data in enumerate(data_generator):

            fit_feed = self.model.inputs.fit_feed(**batch_feed_data)
            _, summaries = self.sess.run([self.model.update, self.summaries], feed_dict=fit_feed)

            if log_summaries and summaries is not None:
                self.writer.add_summary(summaries, global_step=global_step)


            ## REMOVE NEXT, ONLY FOR TESTING
            loss, score = self.sess.run([self.model.loss, self.model.score_tensor], feed_dict=fit_feed)
            print("loss {}, score {}, at {}".format(loss, score, i))
