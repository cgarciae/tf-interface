import tensorflow as tf
from tfinterface.base import Model, RequiredModel
from tfinterface.utils import TENSOR, Required
from tfinterface.metrics import r2_score, sigmoid_score, softmax_score
import cytoolz as cz
import itertools as it
from tfinterface.decorators import return_self, with_graph_as_default
from .supervised_inputs import SupervisedInputs

class RequiredSupervisedModel(RequiredModel): pass
SUPERVISED_MODEL = RequiredSupervisedModel()

class SupervisedModel(Model):
    """
# Inteface
* `inputs : SupervisedInputs` -
* `predictions : Tensor` -
* `[features : Tensor]` -
* `[labels : Tensor]` -
* `[loss : Tensor]` -
* `[update : Tensor]` -
    """

    def __init__(self, name, loss="mse", optimizer=tf.train.AdamOptimizer, learning_rate=0.001, **kwargs):
        super(SupervisedModel, self).__init__(name, **kwargs)

        self._loss_arg = loss
        self._optimizer = optimizer
        self._learning_rate_arg = learning_rate

    def _pre_build(self, inputs, *args, **kwargs):
        pass


    @property
    def inputs_class(self):
        return SupervisedInputs

    def _post_build(self, *args, **kwargs):
        super(SupervisedModel, self)._post_build(*args, **kwargs)

        if not hasattr(self, "labels"):
            self.labels = self.inputs.labels


        if hasattr(self, "learning_rate"):
            learning_rate = self.learning_rate
        elif hasattr(self.inputs, "learning_rate"):
            learning_rate = self.inputs.learning_rate
        else:
            learning_rate = self._learning_rate_arg


        if not hasattr(self, "loss"):
            loss = self._loss_arg

            if loss == "mse":
                self.loss = tf.nn.l2_loss(self.predictions - self.labels) |> tf.reduce_mean
            elif loss == "sigmoid":
                self.loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.labels) |> tf.reduce_mean
            elif loss == "softmax":
                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels) |> tf.reduce_mean


        if not hasattr(self, "update"):
            self.update = self._optimizer(learning_rate).minimize(self.loss)


        if not hasattr(self, "score_tensor"):
            loss = self._loss_arg

            if loss == "mse":
                self.score_tensor = r2_score(self.labels, self.predictions)
            elif loss == "sigmoid":
                self.score_tensor = sigmoid_score(self.labels, self.predictions)
            elif loss == "softmax":
                self.score_tensor = softmax_score(self.labels, self.predictions)



    def predict(self, **kwargs):
        predict_feed = self.inputs.predict_feed(**kwargs)
        return self.sess.run(self.predictions, feed_dict=predict_feed)


    def score(self, **kwargs):
        fit_feed = self.inputs.fit_feed(**kwargs)
        score = self.sess.run(self.score_tensor, feed_dict=fit_feed)

        return score


    @return_self
    @with_graph_as_default
    def fit(self, epochs=2000, data_generator=None, log_summaries=False, writer_kwargs={}):
        if log_summaries and not hasattr(self, "writer"):
            self.writer = tf.summary.FileWriter(self.logs_path, graph=self.graph, **writer_kwargs)

        if not hasattr(self, "summaries"):
            self.summaries = tf.no_op()

        if data_generator is None:
            #generator of empty dicts
            data_generator = it.repeat({})

        data_generator = data_generator |> cz.take$(epochs)

        for i, batch_feed_data in enumerate(data_generator):

            fit_feed = self.inputs.fit_feed(**batch_feed_data)
            _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)

            if log_summaries and summaries is not None:
                self.writer.add_summary(summaries, global_step=global_step)


            ## REMOVE NEXT, ONLY FOR TESTING
            loss, score = self.sess.run([self.loss, self.score_tensor], feed_dict=fit_feed)
            print("loss {}, score {}, at {}".format(loss, score, i))
