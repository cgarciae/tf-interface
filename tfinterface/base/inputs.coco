from .base_class import Base
from tfinterface.decorators import return_self
import tensorflow as tf
from abc import abstractmethod
import threading

##############
# data
##############
data PlaceholderDefaults(tensor, predict, fit)
data NoValue


##############
# exceptions
##############

class NoValueException(Exception):
    pass


##############
# functions
##############

def fit_tuple(PlaceholderDefaults(tensor, _, fit)):
    if isinstance(fit, NoValue):
        raise NoValueException("No fit value given for {}".format(tensor))

    return tensor, fit

def predict_tuple(PlaceholderDefaults(tensor, predict, _)):
    if isinstance(predict, NoValue):
        raise NoValueException("No predict value given for {}".format(tensor))

    return tensor, predict



##############
# classes
##############
class Inputs(Base):

    @abstractmethod
    def fit_feed(self, *args, **kwargs):
        pass

    @abstractmethod
    def predict_feed(self, *args, **kwargs):
        pass

class GeneralInputs(Inputs):
    """docstring for GeneralInputs."""
    def __init__(self, name, graph=None, sess=None, **input_specs):
        super(GeneralInputs, self).__init__(name, graph=graph, sess=sess)
        self._input_specs = input_specs

    @return_self
    def build_tensors(self, **input_overrides):
        input_specs = self._input_specs.copy()
        input_specs.update(input_overrides)
        self._placeholder_defaults = {}

        queue_ops = input_specs.pop("queue_ops", {})

        queued = {}
        for name, spec in input_specs.items():

            if type(spec) is not dict:

                if type(spec) is tuple:
                    spec = dict(dtype=tf.float32, shape=spec)

                elif hasattr(spec, "__call__"):
                    spec = dict(tensor_fn=spec)

                else:
                    spec = dict(value=spec)


            if "queue" in spec:
                queued[name] = spec
                continue

            elif "shape" in spec:
                dtype = spec.get("dtype", tf.float32)
                shape = spec.get("shape")
                tensor = tf.placeholder(dtype=dtype, shape=shape, name=name)

                self._placeholder_defaults[name] = PlaceholderDefaults(
                    tensor,
                    spec.get("predict", NoValue()),
                    spec.get("fit", NoValue())
                )

            elif "value" in spec:
                value = spec.get("value")
                dtype = spec.get("dtype", None)
                tensor = tf.convert_to_tensor(value, dtype=dtype, name=name)


            elif "tensor_fn" in spec:
                tensor_fn = spec.get("tensor_fn")
                tensor = tensor_fn()

            setattr(self, name, tensor)

        if queued:
            self.queue_runner = CustomRunner(self, queued, **queue_ops)

            for name, tensor in self.queue_runner.tensors_dict.items():
                setattr(self, name, tensor)


    def start_queue(self, *args, **kwargs):
        return self.queue_runner.start_threads(*args, **kwargs)

    def get_feed(self, **kwargs):
        return ({
            getattr(self, key) : value for key, value in kwargs.items()
        })

    def _get_fit_defaults(self):
        feed = {}

        for name, placeholder_defaults in self._placeholder_defaults.items():
            try:
                tensor, value = fit_tuple(placeholder_defaults)
                feed[tensor] = value
            except NoValueException as e:
                pass

        return feed

    def _get_predict_defaults(self):
        feed = {}

        for name, placeholder_defaults in self._placeholder_defaults.items():
            try:
                tensor, value = predict_tuple(placeholder_defaults)
                feed[tensor] = value
            except NoValueException as e:
                pass

        return feed

    def fit_feed(self, *args, **kwargs):
        feed = self._get_fit_defaults()
        feed.update(self.get_feed(*args, **kwargs))

        return feed


    def predict_feed(self, *args, **kwargs):
        feed = self._get_predict_defaults()
        feed.update(self.get_feed(*args, **kwargs))

        return feed





class CustomRunner(object):
    """
    This class manages the the background threads needed to fill
        a queue full of data.
    """
    def __init__(self, inputs, queued, batch_size = 64, capacity = 2000, min_after_dequeue = 1000, **queue_ops):
        self.inputs = inputs

        names = names = queued.keys()
        specs = queued.values()
        placeholder_shapes = [ spec.get("shape", [None]) for spec in specs ]
        shapes = [ shape[1:] for shape in placeholder_shapes ]
        dtypes = [ spec.get("dtype", tf.float32) for spec in specs ]

        # placeholders_dict
        self.placeholders_dict = { name: tf.placeholder(dtype=dtype, shape=shape, name = name + "_placeholder") for dtype, shape, name in zip(dtypes, placeholder_shapes, names) }


        # The actual queue of data. The queue contains a vector for
        # the mnist features, and a scalar label.
        self.queue = tf.RandomShuffleQueue(
            capacity,
            min_after_dequeue,
            dtypes,
            shapes = shapes,
            names = names,
            **queue_ops
        )

        # The symbolic operation to add data to the queue
        # we could do some preprocessing here or do it in numpy. In this example
        # we do the scaling in numpy
        self.enqueue_op = self.queue.enqueue_many(self.placeholders_dict)

        # tensors_dict
        self.tensors_dict = self.queue.dequeue_many(batch_size)



    def thread_main(self, data_generator):
        """
        Function run on alternate thread. Basically, keep adding data to the queue.
        """
        for data in data_generator:
            feed_dict = { placeholders_dict[name]: value for name, value in data.items() }
            self.inputs.sess.run(self.enqueue_op, feed_dict = feed_dict)

    def start_queue(self, data_generator, n_threads = 1):
        """ Start background threads to feed queue """
        threads = []
        for n in range(n_threads):
            t = threading.Thread(target=self.thread_main, args=(data_generator,))
            t.daemon = True # thread will close when parent quits
            t.start()
            threads.append(t)
        return threads
