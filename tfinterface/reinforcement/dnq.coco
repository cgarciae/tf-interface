from tfinterface.model_base import ModelBase
from tfinterface.utils import select_columns, soft_if, huber_loss
from .experience_buffer import ExperienceReplay
from rl.policy import Policy, GreedyQPolicy
from rl.agents import DQNAgent
from tfinterface.reinforcement import ExperienceReplay

import numpy as np
from numpy.random import choice
import random
import tensorflow as tf
from scipy.interpolate import interp1d


class DQNEpsPolicy(object):
    def __init__(self, inputs, Qs, nb_actions, eps=1.0, decay_steps=5000,):
        with tf.variable_scope("dqn_policy"):
            batch_size = tf.shape(inputs.s)[0]
            random_eps = tf.random_uniform([batch_size], minval=0, maxval=1, dtype=tf.float32, name="random_eps")
            self.e = tf.train.exponential_decay(eps, inputs.global_step, decay_steps, name="e")
            self.random_action = tf.random_uniform([batch_size], minval=0, maxval=nb_actions, dtype=tf.int32, name="random_action")
            self.greedy_action = tf.a
            self.action = tf.where(
                random_eps < e,
                self.random_action,

            )

class DQNInputs(object):

    def __init__(self, graph, nb_states, batch_size=None, scope="inputs"):

        with tf.name_scope(scope):
            self.s = tf.placeholder(tf.float32, shape=[batch_size, nb_states], name="s")
            self.a = tf.placeholder(tf.int32, shape=[batch_size], name="a")
            self.r = tf.placeholder(tf.float32, shape=[batch_size], name="r")
            self.done = tf.placeholder(tf.bool, shape=[batch_size], name="done")

            self.keep_prob = tf.placeholder(tf.float32, shape=[], name="keep_prob")
            self.training = tf.placeholder(tf.bool, shape=[], name="training")

            self.global_step = tf.Variable(0, trainable=False, name="global_step")


    def predict_feed(self, S):
        return {
            self.s: S,
            self.keep_prob: 1.0,
            self.training: False
        }

    def train_feed(self, S, A, R, Done, keep_prob=0.5, learning_rate=0.001):
        return {
            self.s: S,
            self.a: A,
            self.r: R,
            self.done: Done,

            self.keep_prob: keep_prob,
            self.training: True,
            self.learning_rate: learning_rate
        }


class DQN(ModelBase):

    def define_model(self, model_fn, nb_states, memory=None, policy=None, test_policy=None, enable_double_dqn=True, enable_dueling_network=False,
        dueling_type='avg', gamma=0.99, batch_size=32, nb_steps_warmup=1000,
        train_interval=1, memory_interval=1, target_update=10000,
        delta_range=None, delta_clip=np.inf, scope="dqn", model_scope="model", target_model_scope="target_model", optimizer=tf.train.AdamOptimizer, inputs=None, inputs_class=DQNInputs, memory_max_length=100000, learning_rate=0.001):

        self.memory = memory if memory else ExperienceReplay(4, max_length=memory_max_length)
        self.policy = policy if policy else EpsGreedyQPolicy()
        self.test_policy = test_policy if test_policy else GreedyQPolicy()
        self.gamma = gamma
        self.nb_steps_warmup = nb_steps_warmup
        self.train_interval = train_interval
        self.memory_interval = memory_interval
        self.target_update = target_update
        self.delta_range = delta_range
        self.delta_clip = delta_clip


        with self.graph.as_default():
            self.inputs = inputs_class(self.graph, nb_states, batch_size) if inputs is None else inputs

            with tf.variable_scope(model_scope):
                self.model = model_fn(self.inputs)
                self.model_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=model_scope)

            with tf.variable_scope(target_model_scope):
                self.target_model = model_fn(self.inputs)
                self.target_model_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=target_model_scope)

            with tf.variable_scope(scope):

                self.target_model_target = tf.where(
                    self.inputs.done,
                    self.inputs.r,
                    self.inputs.r + self.gamma * tf.reduce_max(self.target_model.Qs, axis=1)
                )

                self.model_Qsa = select_columns(self.model.Qs, self.inputs.a) if not hasattr(self.model, "Qsa") else self.model.Qsa
                self.model_error = self.target_model_target - self.model_Qsa if not hasattr(self.model, "error") else self.model.error
                self.model_loss = self.model_error |> huber_loss |> tf.reduce_mean if not hasattr(self.model, "loss") else self.model.loss
                self.model_learning_rate = self.model.learning_rate if hasattr(self.model, 'learning_rate') else learning_rate
                self.update = optimizer(self.model_learning_rate).minimize(
                    self.model_loss,
                    var_list=self.model_variables,
                    global_step=self.inputs.global_step
                ) if not hasattr(self.model, "update") else self.model.update

                if self.target_update < 1:
                    self.update = tf.group(self.update, *[
                        tv.assign_add( target_update * (mv - tv) ) for mv, tm in zip(self.target_model_variables, self.model_variables)
                    ])
                    self.update_target_hard = None
                else:
                    self.update_target_hard = tf.group(*[
                        tv.assign( mv ) for mv, tm in zip(self.target_model_variables, self.model_variables)
                    ])


    def predict(self, S, training=False):
        Qs = self.sess.run(self.model.Qs, feed_dict=self.inputs.predict_feed(S))
        policy = self.policy if training else self.test_policy
        return policy.select_action(q_values=Qs)

    def fit(self, env, nb_steps=1000000, keep_prob=0.5, learning_rate=0.001, e=0.1, step=0):

        s = env.reset()

        while step < nb_steps:
            step += 1

            a = self.predict([s], training=True)
            s1, r, done, info = env.step(a)

            if step % self.memory_interval == 0:
                self.memory.append(s, a, r, done)

            if step > self.nb_steps_warmup and step % self.train_interval == 0:
                S, A, R, Done = self.memory.random_batch(self.batch_size).unzip()

                train_feed = self.inputs.train_feed(S, A, R, Done, keep_prob=keep_prob)
                _ = self.sess.run(self.update, feed_dict=train_feed)

            if self.update_target_hard and step % self.target_model_update == 0:
                self.sess.run(self.update_target_hard)

            s = s1
