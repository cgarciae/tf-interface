import tensorflow as tf

#####################################
# batch_norm
#####################################

def dense_batch_norm(*args, **kwargs):

    name = kwargs.pop("name", None)
    activation = kwargs.pop("activation", None)
    bn_kwargs = kwargs.pop("bn_kwargs", {})

    with tf.variable_scope(name, "DenseBatchNorm"):
        net = tf.layers.dense(*args, **kwargs)
        net = tf.layers.batch_normalization(net, **bn_kwargs)

        return activation(net) if activation else net

def conv2d_batch_norm(*args, **kwargs):

    name = kwargs.pop("name", None)
    activation = kwargs.pop("activation", None)
    bn_kwargs = kwargs.pop("bn_kwargs", {})

    with tf.variable_scope(name, default_name="Conv2dBatchNorm"):
        net = tf.layers.conv2d(*args, **kwargs)
        net = tf.layers.batch_normalization(net, **bn_kwargs)

        return activation(net) if activation else net

#####################################
# fire
#####################################
def fire(inputs, squeeze_filters, expand_1x1_filters, expand_3x3_filters, **kwargs):

    name = kwargs.pop("name", None)

    with tf.variable_scope(name, default_name="Fire"):
        # squeeze
        squeeze = tf.layers.conv2d(inputs, squeeze_filters, [1, 1], **kwargs)

        # expand
        kwargs["padding"] = "same"
        expand_1x1 = tf.layers.conv2d(squeeze, expand_1x1_filters, [1, 1], **kwargs)
        expand_3x3 = tf.layers.conv2d(squeeze, expand_3x3_filters, [3, 3], **kwargs)

        return tf.concat([expand_1x1, expand_3x3], axis=3)


def fire_batch_norm(inputs, squeeze_filters, expand_1x1_filters, expand_3x3_filters, **kwargs):

    name = kwargs.pop("name", None)

    with tf.variable_scope(name, default_name="FireBatchNorm"):
        # squeeze
        squeeze = conv2d_batch_norm(inputs, squeeze_filters, [1, 1], **kwargs)

        # expand
        kwargs["padding"] = "same"
        expand_1x1 = conv2d_batch_norm(squeeze, expand_1x1_filters, [1, 1], **kwargs)
        expand_3x3 = conv2d_batch_norm(squeeze, expand_3x3_filters, [3, 3], **kwargs)

        return tf.concat([expand_1x1, expand_3x3], axis=3)


#####################################
# dense_block
#####################################

def conv2d_dense_layer(net, growth_rate, bottleneck, **kwargs):

    inputs = net

    with tf.variable_scope(None, default_name="conv2d_dense_layer"):

        if bottleneck:
            net = tf.layers.conv2d(net, bottleneck, [1, 1], **kwargs)

        net = tf.layers.conv2d(net, growth_rate, [3, 3], **kwargs)

        return tf.concat([inputs, net], axis=3)



def conv2d_dense_block(net, growth_rate, n_layers, **kwargs):
    name = kwargs.pop("name", None)
    bottleneck = kwargs.pop("bottleneck", None)
    compression = kwargs.pop("compression", None)

    with tf.variable_scope(name, default_name="conv2d_dense_block"):
        for layers in range(n_layers):
            net = conv2d_dense_layer(net, growth_rate, bottleneck, **kwargs)

        if compression:
            filters = int(net.get_shape()[-1]) * compression
            filters = int(filters)

            net = tf.layers.conv2d(net, filters, [1, 1], **kwargs)

        return net

if __name__ == '__main__':
    x = tf.random_uniform(shape=(16, 32, 32, 3))

    f = fire(x, 32, 64, 64, activation=tf.nn.relu)
    fb = fire_batch_norm(x, 32, 64, 64, activation=tf.nn.relu, bn_kwargs=dict(training=True))
    print(f)
    print(fb)
